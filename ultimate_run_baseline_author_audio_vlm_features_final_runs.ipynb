{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T23:35:20.482975Z",
     "iopub.status.busy": "2024-10-14T23:35:20.482314Z",
     "iopub.status.idle": "2024-10-14T23:35:25.097097Z",
     "shell.execute_reply": "2024-10-14T23:35:25.096531Z",
     "shell.execute_reply.started": "2024-10-14T23:35:20.482939Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T23:35:25.099003Z",
     "iopub.status.busy": "2024-10-14T23:35:25.098412Z",
     "iopub.status.idle": "2024-10-14T23:35:25.102953Z",
     "shell.execute_reply": "2024-10-14T23:35:25.102328Z",
     "shell.execute_reply.started": "2024-10-14T23:35:25.098980Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Absolute Percentage Error (MAPE) and returns it in percentage format.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        True target values.\n",
    "    - y_pred: Array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Predicted target values.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing MAPE as a percentage (e.g., '32.73%').\n",
    "    \"\"\"\n",
    "    # Import the original sklearn function\n",
    "    from sklearn.metrics import mean_absolute_percentage_error as sklearn_mape\n",
    "    \n",
    "    # Calculate MAPE using sklearn\n",
    "    mape_value = sklearn_mape(y_true, y_pred)\n",
    "    \n",
    "    # Convert to percentage and format as a string\n",
    "    mape_percentage = mape_value * 100\n",
    "    return mape_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T23:35:25.104314Z",
     "iopub.status.busy": "2024-10-14T23:35:25.103903Z",
     "iopub.status.idle": "2024-10-14T23:35:25.107136Z",
     "shell.execute_reply": "2024-10-14T23:35:25.106663Z",
     "shell.execute_reply.started": "2024-10-14T23:35:25.104285Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/notebooks/hackathon/baseline_data\"\n",
    "baseline = \"baseline_author_audio_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T23:35:25.108358Z",
     "iopub.status.busy": "2024-10-14T23:35:25.107993Z",
     "iopub.status.idle": "2024-10-14T23:35:25.166695Z",
     "shell.execute_reply": "2024-10-14T23:35:25.166198Z",
     "shell.execute_reply.started": "2024-10-14T23:35:25.108332Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv(f\"{file_path}/baseline_author_audio_vlm_numerical_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T23:35:25.168589Z",
     "iopub.status.busy": "2024-10-14T23:35:25.168210Z",
     "iopub.status.idle": "2024-10-14T23:35:25.189360Z",
     "shell.execute_reply": "2024-10-14T23:35:25.188831Z",
     "shell.execute_reply.started": "2024-10-14T23:35:25.168567Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Ensure the log directory exists\n",
    "log_dir = '/notebooks/hackathon/logs/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger('model_processing_logger')\n",
    "logger.setLevel(logging.INFO)  # Set the logger level correctly\n",
    "\n",
    "# Check if the logger already has handlers to prevent duplicates\n",
    "if not logger.hasHandlers():\n",
    "    # Create file handler for logging to a file\n",
    "    current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    log_filename = f'{log_dir}final_model_processing_{baseline}_{current_time}.log'\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Create console handler for logging to the console (stdout)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Create a formatter and set it for both handlers\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the handlers to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "# Optional: Disable propagation to avoid duplicate logging if needed\n",
    "logger.propagate = False\n",
    "\n",
    "# Test logging with the custom logger\n",
    "logger.info('Custom logger has been set up successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T23:35:25.190591Z",
     "iopub.status.busy": "2024-10-14T23:35:25.190253Z",
     "iopub.status.idle": "2024-10-14T23:35:25.211233Z",
     "shell.execute_reply": "2024-10-14T23:35:25.210703Z",
     "shell.execute_reply.started": "2024-10-14T23:35:25.190569Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "numerical_features = ['author_follower_count',\n",
    "       'author_following_count', 'author_total_heart_count',\n",
    "       'author_total_video_count', 'follower_following_ratio',\n",
    "       'avg_hearts_per_video', 'engagement_rate', 'videos_per_follower',\n",
    "       'Video_Length', 'Hashtags', 'Collaborations', 'Series', 'Post_Day',\n",
    "       'Post_Hour', 'Post_Month', 'Post_Season', 'Post_Quarter',\n",
    "       'Post_Week_of_Year', 'Post_Part_of_Day', 'Is_Weekend',\n",
    "       'Next_Day_Holiday', 'Is_Long_Weekend', 'Engagement_Rate',\n",
    "       'Description_Sentiment', 'industry', 'Sentiment Score',\n",
    "       'Speech vs. Action Focus', 'Setting', 'Interaction Type', 'Tone',\n",
    "       'Dialogue/Monologue', 'Audience Engagement',\n",
    "       'zero_crossing_rate_mean', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4',\n",
    "       'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11',\n",
    "       'mfcc_12', 'mfcc_13', 'spectral_contrast_1', 'spectral_contrast_2',\n",
    "       'spectral_contrast_3', 'spectral_contrast_4', 'spectral_contrast_5',\n",
    "       'spectral_contrast_6', 'spectral_contrast_7', 'chroma_1', 'chroma_2',\n",
    "       'chroma_3', 'chroma_4', 'chroma_5', 'chroma_6', 'chroma_7', 'chroma_8',\n",
    "       'chroma_9', 'chroma_10', 'chroma_11', 'chroma_12']\n",
    "\n",
    "\n",
    "text_features = ['video_description', 'transcribe_text', 'generated_vlm_text', \"combined_text\"]\n",
    "\n",
    "labels = ['video_comment_count', 'video_heart_count', 'video_play_count', 'video_share_count']\n",
    "\n",
    "# Fill NaN values with \"empty input\" for each text column\n",
    "df_merged[\"video_description\"] = df_merged[\"video_description\"].fillna(\"empty input\").astype(str)\n",
    "df_merged[\"transcribe_text\"] = df_merged[\"transcribe_text\"].fillna(\"empty input\").astype(str)\n",
    "df_merged[\"generated_vlm_text\"] = df_merged[\"generated_vlm_text\"].fillna(\"empty input\").astype(str)\n",
    "\n",
    "# Alternatively, using agg for better readability and flexibility\n",
    "df_merged[\"combined_text\"] = df_merged[[\"video_description\", \"transcribe_text\", \"generated_vlm_text\"]].astype(str).agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T23:35:25.212442Z",
     "iopub.status.busy": "2024-10-14T23:35:25.212082Z",
     "iopub.status.idle": "2024-10-14T23:35:25.222630Z",
     "shell.execute_reply": "2024-10-14T23:35:25.222062Z",
     "shell.execute_reply.started": "2024-10-14T23:35:25.212421Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Selecting categorical columns to encode\n",
    "categorical_columns = ['Collaborations', 'Series', 'Post_Day', 'Post_Month', 'Post_Season', 'Post_Part_of_Day', \"Post_Part_of_Day\", 'Is_Weekend',\n",
    "       'Next_Day_Holiday', 'Is_Long_Weekend', 'industry', 'Sentiment Score', 'Speech vs. Action Focus', 'Setting', 'Interaction Type', 'Tone', \n",
    "       'Dialogue/Monologue', 'Audience Engagement']\n",
    "\n",
    "# Encoding categorical columns\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_merged[column] = le.fit_transform(df_merged[column])\n",
    "    label_encoders[column] = le  # Storing the encoder for each column in case you need to reverse the transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T23:35:25.223808Z",
     "iopub.status.busy": "2024-10-14T23:35:25.223463Z",
     "iopub.status.idle": "2024-10-14T23:35:25.228916Z",
     "shell.execute_reply": "2024-10-14T23:35:25.228348Z",
     "shell.execute_reply.started": "2024-10-14T23:35:25.223785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define MAPE Loss Function\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        epsilon = 1e-10  # A small value to avoid division by zero\n",
    "        loss = torch.abs((target - preds) / (target + epsilon))\n",
    "        return torch.mean(loss) * 100\n",
    "    \n",
    "# Define SMAPE Loss Function\n",
    "class SMAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SMAPELoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        epsilon = 1e-10\n",
    "        smape = 200 * torch.mean(torch.abs(targets - preds) / (torch.abs(targets) + torch.abs(preds) + epsilon))\n",
    "        return smape\n",
    "\n",
    "# Define MAE Loss Function\n",
    "class MAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        return torch.mean(torch.abs(targets - preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is in a pandas DataFrame `df` with features in X and target in y\n",
    "X = df_merged[numerical_features]\n",
    "y = df_merged[labels[0]].values\n",
    "\n",
    "\n",
    "# Step 2: Initialize KFold for the cross-validation step on the 80% (or 90%) train set\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom Dataset for Regression Task\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Transformer-based Model\n",
    "class TransformerRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8, hidden_dim=64, num_layers=2):\n",
    "        super(TransformerRegressionModel, self).__init__()\n",
    "        \n",
    "        if input_dim < 8:\n",
    "            input_dim = 8  # Set a minimum threshold\n",
    "            logger.info(f\"Input_dim is too small, adjusting to minimum threshold: {input_dim}\")\n",
    "        \n",
    "        if input_dim % num_heads != 0:\n",
    "            self.embedding_dim = (input_dim // num_heads) * num_heads\n",
    "            logger.info(f\"Projecting input_dim from {input_dim} to {self.embedding_dim} to be divisible by num_heads={num_heads}\")\n",
    "        else:\n",
    "            self.embedding_dim = input_dim\n",
    "        \n",
    "        self.fc_embedding = nn.Linear(input_dim, self.embedding_dim)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc_embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        x = self.relu(self.fc1(attn_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Placeholder for storing test MAPE for each fold\n",
    "test_mape_scores = []\n",
    "\n",
    "test_mape_scores = []\n",
    "fold_train_losses = []\n",
    "fold_val_losses = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    logger.info(f\"\\nFold {fold + 1}\")\n",
    "    \n",
    "    # Use iloc to ensure you are selecting rows by their positional index\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_test = y[train_idx], y[val_idx]\n",
    "\n",
    "    X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = RegressionDataset(X_train_val, y_train_val)\n",
    "    val_dataset = RegressionDataset(X_val, y_val)\n",
    "    test_dataset = RegressionDataset(X_test, y_test)  # Test set remains fixed\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize Model\n",
    "    input_dim = X.shape[1]  # Number of features\n",
    "    model = TransformerRegressionModel(input_dim=input_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    loss_type = \"Huber\"\n",
    "    if loss_type == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_type == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif loss_type == \"Huber\":\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "    elif loss_type == \"MAPE\":\n",
    "        loss_fn = MAPELoss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses_epoch = []\n",
    "    val_losses_epoch = []\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        train_losses_epoch.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = loss_fn(outputs, batch_y)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_losses_epoch.append(avg_val_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            logger.info(f'Epoch {epoch+1}, Fold {fold+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Store fold-wise losses\n",
    "    fold_train_losses.append(train_losses_epoch)\n",
    "    fold_val_losses.append(val_losses_epoch)\n",
    "    \n",
    "    # Test the model on the hold-out test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            preds = model(batch_X)\n",
    "            all_preds.extend(preds.squeeze(1).tolist())\n",
    "\n",
    "    # Compute MAPE on the test set\n",
    "    test_mape = mean_absolute_percentage_error(y_test, all_preds)\n",
    "    test_mape_scores.append(test_mape)\n",
    "\n",
    "# Plot training and validation losses for each fold\n",
    "for fold in range(kf.get_n_splits()):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fold_train_losses[fold], label='Train Loss')\n",
    "    plt.plot(fold_val_losses[fold], label='Validation Loss')\n",
    "    plt.title(f'Fold {fold+1} Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the average training and validation losses across all folds\n",
    "avg_train_losses = np.mean(fold_train_losses, axis=0)\n",
    "avg_val_losses = np.mean(fold_val_losses, axis=0)\n",
    "\n",
    "# Plot the average training and validation losses across all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_train_losses, label='Average Train Loss', color='blue')\n",
    "plt.plot(avg_val_losses, label='Average Validation Loss', color='orange')\n",
    "plt.title(f' label {labels[0]} - Average Loss Curves Across All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "# Plot test MAPE scores across all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, kf.get_n_splits() + 1), test_mape_scores, marker='o', label='Test MAPE')\n",
    "plt.title('Test MAPE Scores Across Folds')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('MAPE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mean_mape = np.mean(test_mape_scores)\n",
    "std_mape = np.std(test_mape_scores)\n",
    "\n",
    "logger.info(f'MAPE across all folds on the {labels[0]} test set: {mean_mape:.4f}')\n",
    "logger.info(f'Average MAPE across all folds on the {labels[0]} test set: {mean_mape:.4f}')\n",
    "logger.info(f'Standard deviation of MAPE across all folds on the {labels[0]} test set: {std_mape:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Assuming your data is in a pandas DataFrame `df_merged` with features in X and target in y\n",
    "X = df_merged[numerical_features]\n",
    "y = df_merged[labels[0]].values\n",
    "\n",
    "# Step 1: Standardize features (X)\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Step 2: Standardize target variable (y)\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()  # Standardizing y\n",
    "\n",
    "# Step 3: Initialize KFold for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom Dataset for Regression Task\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Transformer-based Model\n",
    "class TransformerRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8, hidden_dim=64, num_layers=2):\n",
    "        super(TransformerRegressionModel, self).__init__()\n",
    "        \n",
    "        if input_dim < 8:\n",
    "            input_dim = 8  # Set a minimum threshold\n",
    "            logger.info(f\"Input_dim is too small, adjusting to minimum threshold: {input_dim}\")\n",
    "        \n",
    "        if input_dim % num_heads != 0:\n",
    "            self.embedding_dim = (input_dim // num_heads) * num_heads\n",
    "            logger.info(f\"Projecting input_dim from {input_dim} to {self.embedding_dim} to be divisible by num_heads={num_heads}\")\n",
    "        else:\n",
    "            self.embedding_dim = input_dim\n",
    "        \n",
    "        self.fc_embedding = nn.Linear(input_dim, self.embedding_dim)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc_embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        x = self.relu(self.fc1(attn_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Placeholder for storing test MAPE for each fold\n",
    "test_mape_scores = []\n",
    "\n",
    "fold_train_losses = []\n",
    "fold_val_losses = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    logger.info(f\"\\nFold {fold + 1}\")\n",
    "    \n",
    "    # Use iloc to ensure you are selecting rows by their positional index\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_test = y_scaled[train_idx], y_scaled[val_idx]  # Use scaled y\n",
    "\n",
    "    X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = RegressionDataset(X_train_val, y_train_val)\n",
    "    val_dataset = RegressionDataset(X_val, y_val)\n",
    "    test_dataset = RegressionDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize Model\n",
    "    input_dim = X.shape[1]  # Number of features\n",
    "    model = TransformerRegressionModel(input_dim=input_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Choose loss function (example: MSE)\n",
    "    loss_type = \"MSE\"\n",
    "    if loss_type == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_type == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif loss_type == \"Huber\":\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "    elif loss_type == \"MAPE\":\n",
    "        loss_fn = MAPELoss()\n",
    "    elif loss_type == \"SMAPE\":\n",
    "        loss_fn = SMAPELoss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses_epoch = []\n",
    "    val_losses_epoch = []\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        train_losses_epoch.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = loss_fn(outputs, batch_y)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_losses_epoch.append(avg_val_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            logger.info(f'Epoch {epoch+1}, Fold {fold+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Store fold-wise losses\n",
    "    fold_train_losses.append(train_losses_epoch)\n",
    "    fold_val_losses.append(val_losses_epoch)\n",
    "    \n",
    "    # Test the model on the hold-out test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            preds = model(batch_X)\n",
    "            all_preds.extend(preds.tolist())  # Collect predictions\n",
    "\n",
    "    # Inverse scaling for predictions\n",
    "    all_preds = scaler_y.inverse_transform(np.array(all_preds).reshape(-1, 1)).flatten()  # Reverse scaling\n",
    "\n",
    "    # Compute MAPE on the test set\n",
    "    test_mape = mean_absolute_percentage_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), all_preds)\n",
    "    test_mape_scores.append(test_mape)\n",
    "\n",
    "# Plot training and validation losses for each fold\n",
    "for fold in range(kf.get_n_splits()):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fold_train_losses[fold], label='Train Loss')\n",
    "    plt.plot(fold_val_losses[fold], label='Validation Loss')\n",
    "    plt.title(f'Fold {fold+1} Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the average training and validation losses across all folds\n",
    "avg_train_losses = np.mean(fold_train_losses, axis=0)\n",
    "avg_val_losses = np.mean(fold_val_losses, axis=0)\n",
    "\n",
    "# Plot the average training and validation losses across all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_train_losses, label='Average Train Loss', color='blue')\n",
    "plt.plot(avg_val_losses, label='Average Validation Loss', color='orange')\n",
    "plt.title(f' label {labels[0]} - Average Loss Curves Across All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot test MAPE scores across all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, kf.get_n_splits() + 1), test_mape_scores, marker='o', label='Test MAPE')\n",
    "plt.title('Test MAPE Scores Across Folds')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('MAPE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mean_mape = np.mean(test_mape_scores)\n",
    "std_mape = np.std(test_mape_scores)\n",
    "\n",
    "logger.info(f'MAPE across all folds on the {labels[0]} test set: {mean_mape:.4f}')\n",
    "logger.info(f'Average MAPE across all folds on the {labels[0]} test set: {mean_mape:.4f}')\n",
    "logger.info(f'Standard deviation of MAPE across all folds on the {labels[0]} test set: {std_mape:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Assuming your data is in a pandas DataFrame `df_merged` with features in X and target in y\n",
    "X = df_merged[numerical_features]\n",
    "y = df_merged[labels[0]].values\n",
    "\n",
    "# Step 1: Apply Min-Max Scaling to features (X)\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply Min-Max Scaling to target variable (y)\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 3: Initialize KFold for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom Dataset for Regression Task\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Transformer-based Model\n",
    "class TransformerRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8, hidden_dim=64, num_layers=2):\n",
    "        super(TransformerRegressionModel, self).__init__()\n",
    "        \n",
    "        if input_dim < 8:\n",
    "            input_dim = 8  # Set a minimum threshold\n",
    "            logger.info(f\"Input_dim is too small, adjusting to minimum threshold: {input_dim}\")\n",
    "        \n",
    "        if input_dim % num_heads != 0:\n",
    "            self.embedding_dim = (input_dim // num_heads) * num_heads\n",
    "            logger.info(f\"Projecting input_dim from {input_dim} to {self.embedding_dim} to be divisible by num_heads={num_heads}\")\n",
    "        else:\n",
    "            self.embedding_dim = input_dim\n",
    "        \n",
    "        self.fc_embedding = nn.Linear(input_dim, self.embedding_dim)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc_embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        x = self.relu(self.fc1(attn_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Placeholder for storing test MAPE for each fold\n",
    "test_mape_scores = []\n",
    "\n",
    "fold_train_losses = []\n",
    "fold_val_losses = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    logger.info(f\"\\nFold {fold + 1}\")\n",
    "    \n",
    "    # Split the data based on index from the KFold split\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_test = y_scaled[train_idx], y_scaled[val_idx]  # Use scaled y\n",
    "\n",
    "    X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = RegressionDataset(X_train_val, y_train_val)\n",
    "    val_dataset = RegressionDataset(X_val, y_val)\n",
    "    test_dataset = RegressionDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    input_dim = X.shape[1]  # Number of features\n",
    "    model = TransformerRegressionModel(input_dim=input_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Choose loss function (e.g., MSE)\n",
    "    loss_type = \"MSE\"\n",
    "    if loss_type == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_type == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif loss_type == \"Huber\":\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "    elif loss_type == \"MAPE\":\n",
    "        loss_fn = MAPELoss()\n",
    "    elif loss_type == \"SMAPE\":\n",
    "        loss_fn = SMAPELoss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses_epoch = []\n",
    "    val_losses_epoch = []\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        train_losses_epoch.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = loss_fn(outputs, batch_y)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_losses_epoch.append(avg_val_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            logger.info(f'Epoch {epoch+1}, Fold {fold+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Store fold-wise losses\n",
    "    fold_train_losses.append(train_losses_epoch)\n",
    "    fold_val_losses.append(val_losses_epoch)\n",
    "    \n",
    "    # Test the model on the hold-out test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            preds = model(batch_X)\n",
    "            all_preds.extend(preds.tolist())  # Collect predictions\n",
    "\n",
    "    # Inverse scaling for predictions\n",
    "    all_preds = scaler_y.inverse_transform(np.array(all_preds).reshape(-1, 1)).flatten()  # Reverse scaling\n",
    "\n",
    "    # Compute MAPE on the test set\n",
    "    test_mape = mean_absolute_percentage_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), all_preds)\n",
    "    test_mape_scores.append(test_mape)\n",
    "\n",
    "# Plot training and validation losses for each fold\n",
    "for fold in range(kf.get_n_splits()):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fold_train_losses[fold], label='Train Loss')\n",
    "    plt.plot(fold_val_losses[fold], label='Validation Loss')\n",
    "    plt.title(f'Fold {fold+1} Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the average training and validation losses across all folds\n",
    "avg_train_losses = np.mean(fold_train_losses, axis=0)\n",
    "avg_val_losses = np.mean(fold_val_losses, axis=0)\n",
    "\n",
    "# Plot the average training and validation losses across all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_train_losses, label='Average Train Loss', color='blue')\n",
    "plt.plot(avg_val_losses, label='Average Validation Loss', color='orange')\n",
    "plt.title(f' label {labels[0]} - Average Loss Curves Across All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot test MAPE scores across all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, kf.get_n_splits() + 1), test_mape_scores, marker='o', label='Test MAPE')\n",
    "plt.title('Test MAPE Scores Across Folds')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('MAPE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mean_mape = np.mean(test_mape_scores)\n",
    "std_mape = np.std(test_mape_scores)\n",
    "\n",
    "logger.info(f'MAPE across all folds on the {labels[0]} test set: {mean_mape:.4f}')\n",
    "logger.info(f'Average MAPE across all folds on the {labels[0]} test set: {mean_mape:.4f}')\n",
    "logger.info(f'Standard deviation of MAPE across all folds on the {labels[0]} test set: {std_mape:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Assuming your data is in a pandas DataFrame `df_merged` with features in X and target in y\n",
    "X = df_merged[numerical_features]\n",
    "y = df_merged[labels[0]].values\n",
    "\n",
    "# Step 1: Apply Min-Max Scaling to features (X)\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply Min-Max Scaling to target variable (y)\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 3: Initialize KFold for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom Dataset for Regression Task\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Transformer-based Model\n",
    "class TransformerRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8, hidden_dim=64, num_layers=2):\n",
    "        super(TransformerRegressionModel, self).__init__()\n",
    "        \n",
    "        if input_dim < 8:\n",
    "            input_dim = 8  # Set a minimum threshold\n",
    "            logger.info(f\"Input_dim is too small, adjusting to minimum threshold: {input_dim}\")\n",
    "        \n",
    "        if input_dim % num_heads != 0:\n",
    "            self.embedding_dim = (input_dim // num_heads) * num_heads\n",
    "            logger.info(f\"Projecting input_dim from {input_dim} to {self.embedding_dim} to be divisible by num_heads={num_heads}\")\n",
    "        else:\n",
    "            self.embedding_dim = input_dim\n",
    "        \n",
    "        self.fc_embedding = nn.Linear(input_dim, self.embedding_dim)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc_embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        x = self.relu(self.fc1(attn_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Placeholder for storing test MAPE for each fold\n",
    "test_mape_scores = []\n",
    "\n",
    "fold_train_losses = []\n",
    "fold_val_losses = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    logger.info(f\"\\nFold {fold + 1}\")\n",
    "    \n",
    "    # Split the data based on index from the KFold split\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_test = y_scaled[train_idx], y_scaled[val_idx]  # Use scaled y\n",
    "\n",
    "    X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = RegressionDataset(X_train_val, y_train_val)\n",
    "    val_dataset = RegressionDataset(X_val, y_val)\n",
    "    test_dataset = RegressionDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    input_dim = X.shape[1]  # Number of features\n",
    "    model = TransformerRegressionModel(input_dim=input_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Choose loss function (e.g., MSE)\n",
    "    loss_type = \"Huber\"\n",
    "    if loss_type == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_type == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif loss_type == \"Huber\":\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "    elif loss_type == \"MAPE\":\n",
    "        loss_fn = MAPELoss()\n",
    "    elif loss_type == \"SMAPE\":\n",
    "        loss_fn = SMAPELoss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses_epoch = []\n",
    "    val_losses_epoch = []\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        train_losses_epoch.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = loss_fn(outputs, batch_y)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_losses_epoch.append(avg_val_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            logger.info(f'Epoch {epoch+1}, Fold {fold+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Store fold-wise losses\n",
    "    fold_train_losses.append(train_losses_epoch)\n",
    "    fold_val_losses.append(val_losses_epoch)\n",
    "    \n",
    "    # Test the model on the hold-out test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            preds = model(batch_X)\n",
    "            all_preds.extend(preds.tolist())  # Collect predictions\n",
    "\n",
    "    # Inverse scaling for predictions\n",
    "    all_preds = scaler_y.inverse_transform(np.array(all_preds).reshape(-1, 1)).flatten()  # Reverse scaling\n",
    "\n",
    "    # Compute MAPE on the test set\n",
    "    test_mape = mean_absolute_percentage_error(scaler_y.inverse_transform(y_test.reshape(-1, 1)), all_preds)\n",
    "    test_mape_scores.append(test_mape)\n",
    "\n",
    "# Plot training and validation losses for each fold\n",
    "for fold in range(kf.get_n_splits()):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fold_train_losses[fold], label='Train Loss')\n",
    "    plt.plot(fold_val_losses[fold], label='Validation Loss')\n",
    "    plt.title(f'Fold {fold+1} Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the average training and validation losses across all folds\n",
    "avg_train_losses = np.mean(fold_train_losses, axis=0)\n",
    "avg_val_losses = np.mean(fold_val_losses, axis=0)\n",
    "\n",
    "# Plot the average training and validation losses across all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_train_losses, label='Average Train Loss', color='blue')\n",
    "plt.plot(avg_val_losses, label='Average Validation Loss', color='orange')\n",
    "plt.title(f' label {labels[0]} - Average Loss Curves Across All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot test MAPE scores across all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, kf.get_n_splits() + 1), test_mape_scores, marker='o', label='Test MAPE')\n",
    "plt.title('Test MAPE Scores Across Folds')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('MAPE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mean_mape = np.mean(test_mape_scores)\n",
    "std_mape = np.std(test_mape_scores)\n",
    "\n",
    "logger.info(f'MAPE across all folds on the {labels[0]} test set: {mean_mape:.4f}')\n",
    "logger.info(f'Average MAPE across all folds on the {labels[0]} test set: {mean_mape:.4f}')\n",
    "logger.info(f'Standard deviation of MAPE across all folds on the {labels[0]} test set: {std_mape:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
